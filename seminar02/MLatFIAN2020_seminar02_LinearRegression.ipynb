{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLatFIAN2020-seminar02-LinearRegression.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPXBSDL2alJsFvHgcyQs3Bt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HSE-LAMBDA/MLatFIAN2020/blob/master/seminar02/MLatFIAN2020_seminar02_LinearRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bJ2JqbrfLfj",
        "colab_type": "text"
      },
      "source": [
        "# Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDvcL1FjsZLY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHoDZiwJsHlW",
        "colab_type": "text"
      },
      "source": [
        "## y = kx + b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8DvsrrgfPxC",
        "colab_type": "text"
      },
      "source": [
        "Let's start with a toy 1D problem, where the true dependence is\n",
        "$$y=k\\cdot x+b+\\text{noise}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKRgT8hyxRm_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def linear_function(x):\n",
        "  return 0.33 * x + 8.3\n",
        "\n",
        "def gen_dataset(N, func, lims=(-1., 1.), noise_lvl=0.2):\n",
        "  x = np.random.uniform(*lims, size=N)\n",
        "  y = func(x) + noise_lvl * np.random.normal(size=x.shape)\n",
        "  return x[:,None], y\n",
        "\n",
        "X, y = gen_dataset(50, linear_function)\n",
        "x = np.linspace(-1, 1, 101)\n",
        "plt.plot(x, linear_function(x))\n",
        "plt.scatter(X, y);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bG5zspXRrYWL",
        "colab_type": "text"
      },
      "source": [
        "### `LinearRegression` from `sklearn`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92YYaRU9e2jz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The following class implements the analytical solution for\n",
        "# linear regression with the MSE loss\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "model = LinearRegression()\n",
        "\n",
        "model.fit(X, y)\n",
        "\n",
        "x = np.linspace(-1, 1, 101)\n",
        "plt.plot(x, linear_function(x), label='true function')\n",
        "plt.scatter(X, y);\n",
        "plt.plot(x, model.predict(x[:,None]), label='prediction')\n",
        "plt.legend()\n",
        "\n",
        "print(model.coef_, model.intercept_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WGS--_XrSe9",
        "colab_type": "text"
      },
      "source": [
        "### Sidenote: making contour plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndqMFPx6oTpP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Sidenote: making contour plots (level maps)\n",
        "\n",
        "plt.contourf(\n",
        "    [[0., 1., 2.], # matrix of X\n",
        "     [0., 1., 2.],\n",
        "     [0., 1., 2.]],\n",
        "    [[ 0.,  0.,  0.], # matrix of Y\n",
        "     [10., 10., 10.],\n",
        "     [20., 20., 20.]],\n",
        "    [[-1., 0., -1.], # matrix of Z\n",
        "     [ 0., 1.,  0.],\n",
        "     [-1., 0., -1.]]\n",
        ");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQs61KUCqV5G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 2D matrices of X and Y (as above) can be\n",
        "### created from 1D vectors using np.meshgrid:\n",
        "\n",
        "for i in np.meshgrid([0., 1., 2], [0., 10., 20.]):\n",
        "  print(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-PAHAotrkJV",
        "colab_type": "text"
      },
      "source": [
        "### MSE as a function of model parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFQzedCIst18",
        "colab_type": "text"
      },
      "source": [
        "Let's see what MSE looks like as a function of model parameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNT3O5soiWwb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating a grid of model parameter values:\n",
        "ww, bb = np.meshgrid(\n",
        "    np.linspace(-10., 10., 50),\n",
        "    np.linspace(-5., 15., 50)\n",
        ")\n",
        "\n",
        "# Your turn: calculate the map of MSE values on the grid defined above, i.e.\n",
        "# for each (w, b) in (ww, bb) calculate MSE for the model y = w * x + b.\n",
        "# Avoid using loops.\n",
        "MSE_map = <YOUR CODE>\n",
        "\n",
        "# Automatic checks\n",
        "assert MSE_map.shape == ww.shape\n",
        "for i in [0, -1]:\n",
        "  for j in [0, -1]:\n",
        "    assert np.isclose(\n",
        "        MSE_map[i, j],\n",
        "        ((ww[i, j] * X.ravel() + bb[i, j] - y)**2).mean()\n",
        "    ), f'assert failed for point {i, j}'\n",
        "\n",
        "# Plotting:\n",
        "plt.figure(figsize=(6, 5), dpi=100)\n",
        "plt.colorbar(plt.contourf(ww, bb, MSE_map, levels=30))\n",
        "plt.scatter(model.coef_, model.intercept_, marker='*', s=150, c='orange');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KA_a_UsFsYPR",
        "colab_type": "text"
      },
      "source": [
        "## Polynomial fit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCSdUOg__u0n",
        "colab_type": "text"
      },
      "source": [
        "Now let's take some arbitrary function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6X2splyeIVH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def true_function(x):\n",
        "  return np.sin(3 * x + 0.8) + np.sin(1. / (x + 1.23))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPgBZq4l_0Ex",
        "colab_type": "text"
      },
      "source": [
        "Obviously, we won't get a good fit with an ordinary linear regression:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmSBngOE0JdP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = LinearRegression()\n",
        "X, y = gen_dataset(25, true_function)\n",
        "\n",
        "model.fit(X, y)\n",
        "\n",
        "x = np.linspace(-1, 1, 101)\n",
        "plt.plot(x, true_function(x), label='true function')\n",
        "plt.scatter(X, y);\n",
        "plt.plot(x, model.predict(x[:,None]), label='prediction')\n",
        "plt.legend();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXhZyytaANX2",
        "colab_type": "text"
      },
      "source": [
        "### `PolynomialFeatures` and pipelines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbESfwRuAZD7",
        "colab_type": "text"
      },
      "source": [
        "Even though our design matrix has only one column:\n",
        "$$X=\n",
        "\\begin{pmatrix}\n",
        "x_1 \\\\\n",
        "x_2 \\\\\n",
        "\\vdots \\\\\n",
        "x_N\n",
        "\\end{pmatrix},\n",
        "$$\n",
        "we can expand it with powers of $x$ to fit a polynomial:\n",
        "$$X'=\n",
        "\\begin{pmatrix}\n",
        "x_1 & (x_1)^2 & \\ldots & (x_1)^k \\\\\n",
        "x_2 & (x_2)^2 & \\ldots & (x_2)^k \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "x_N & (x_N)^2 & \\ldots & (x_N)^k\n",
        "\\end{pmatrix},\n",
        "$$\n",
        "\n",
        "such that:\n",
        "\n",
        "$$\\frac{1}{N}\\left\\Vert X'\\cdot w - y\\right\\Vert^2\\to \\underset{w}{\\text{min}}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBX4E80aDfBv",
        "colab_type": "text"
      },
      "source": [
        "This functionality is implemented in `sklearn.preprocessing.PolynomialFeatures`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tnvqh81sDeJb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "poly_expand = PolynomialFeatures(3)\n",
        "poly_expand.fit_transform(np.arange(5)[:,None])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tD4hq4nVE0WO",
        "colab_type": "text"
      },
      "source": [
        "One can combine `PolynomialFeatures` (and any other transformers) along with the model into a single pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtNLILIP25-U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.pipeline import make_pipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXG8osw43CI-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The first parameter is the power of expansion. Try playing around with it.\n",
        "poly_expand = PolynomialFeatures(5, include_bias=False)\n",
        "linear_model = LinearRegression()\n",
        "model = make_pipeline(\n",
        "    poly_expand, linear_model\n",
        ")\n",
        "\n",
        "model.fit(X, y)\n",
        "\n",
        "x = np.linspace(-1, 1, 101)\n",
        "plt.plot(x, true_function(x), label='true function')\n",
        "plt.scatter(X, y);\n",
        "plt.plot(x, model.predict(x[:,None]), label='prediction')\n",
        "plt.ylim(y.min() - 0.5, y.max() + 0.5)\n",
        "plt.legend();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiWLsor3c2hp",
        "colab_type": "text"
      },
      "source": [
        "Now we want to plot 2D projections of MSE as a function of model parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTXh8fe9-gXE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import trange, tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xZFirLY3hHj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Combine the weights and the bias into a single parameter vector\n",
        "solution = np.concatenate([linear_model.coef_, [linear_model.intercept_]])\n",
        "\n",
        "# Calculate the power expansion of the features\n",
        "X_expanded = np.concatenate([\n",
        "    poly_expand.transform(X), np.ones(shape=(len(X), 1))\n",
        "], axis=1)\n",
        "\n",
        "# We'll plot a large matrix of plot, so let's create\n",
        "# a 16 by 16 inch canvas\n",
        "plt.figure(figsize=(16, 16))\n",
        "\n",
        "# We'll loop over all pairs of weights\n",
        "i_img = 0\n",
        "for dim1 in trange(len(solution)):\n",
        "  for dim2 in range(len(solution)):\n",
        "    i_img += 1\n",
        "    # Skip the diagonal\n",
        "    if dim1 == dim2: continue\n",
        "\n",
        "    # Create the grid of parameter values\n",
        "    ww1, ww2 = np.meshgrid(\n",
        "        np.linspace(solution[dim1] - 1000., solution[dim1] + 1000., 50),\n",
        "        np.linspace(solution[dim2] - 1000., solution[dim2] + 1000., 50),\n",
        "    )\n",
        "\n",
        "    # Your turn! To calculate the map of MSE values, let's first\n",
        "    # create `param_grid` - a 3D array of parameter values of the\n",
        "    # following shape: (len(solution), ww1.shape[0], ww1.shape[1])\n",
        "    #\n",
        "    # I.e. `param_grid[i, :, :]` should equal to:\n",
        "    #     `ww1` if `i` equals `dim1`;\n",
        "    #     `ww2` if `i` equals `dim2`;\n",
        "    #     `solution[i]` otherwise.\n",
        "    \n",
        "    param_grid = <YOUR CODE>\n",
        "\n",
        "    # Automatic checks\n",
        "    assert param_grid.shape == (len(solution),) + ww1.shape\n",
        "    assert np.allclose(param_grid[dim1], ww1)\n",
        "    assert np.allclose(param_grid[dim2], ww2)\n",
        "    assert all(\n",
        "        np.allclose(param_grid[i], solution[i])\n",
        "        for i in range(len(solution)) if i not in (dim1, dim2)\n",
        "    )\n",
        "\n",
        "    # Your turn! Now it's time to calculate the MSE map, i.e. for each grid\n",
        "    # element (i, j), you want `MSE_map[i, j]` to be equal to the MSE\n",
        "    # for the model defined by parameters `param_grid[:, i, j]`.\n",
        "    MSE_map = <YOUR CODE>\n",
        "\n",
        "    # Automatic checks\n",
        "    assert MSE_map.shape == ww1.shape\n",
        "    for i in [0, -1]:\n",
        "      for j in [0, -1]:\n",
        "        assert np.isclose(\n",
        "            ((X_expanded @ param_grid[:, i, j] - y)**2).mean(),\n",
        "            MSE_map[i, j]\n",
        "        ), f'Check failed for point {i, j}'\n",
        "\n",
        "    plt.subplot(len(solution), len(solution), i_img)\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.contourf(ww1, ww2, MSE_map, levels=10);\n",
        "    plt.scatter(solution[dim1], solution[dim2], marker='*', s=30, c='orange')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUbyj4uLMsMp",
        "colab_type": "text"
      },
      "source": [
        "Note the relation between the amount of overfitting and correlation between parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6Md8oRbHNJG",
        "colab_type": "text"
      },
      "source": [
        "## Gradient descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aj-ZaSJCifkV",
        "colab_type": "text"
      },
      "source": [
        "Let's look at MSE as a function of parameters:\n",
        "$$\\text{MSE}(w)=\\frac{1}{N}\\left\\Vert X'\\cdot w - y\\right\\Vert^2$$\n",
        "\n",
        "Instead of minimizing it analytically, we can use numeric optimization with gradient descent. I.e. do the following procedure iteratively:\n",
        "$$w\\leftarrow w-\\alpha\\cdot\\frac{\\partial\\text{MSE}(w)}{\\partial w},$$\n",
        "for some constant *learning rate* $\\alpha$.\n",
        "\n",
        "For the task below you'll need to derive the analytical formula for $\\frac{\\partial\\text{MSE}(w)}{\\partial w}$. **Note, that $w$ is a vector!** If not sure how to do it, check out the [matrix calculus cheat sheet](https://en.wikipedia.org/wiki/Matrix_calculus#Identities).\n",
        "\n",
        "When done, play around with the power of the polynomial expansion, learning rate and the number of gradient descent steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AERh5jc-tBT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize the model parameters with zeros\n",
        "w = np.zeros(dtype=float, shape=X_expanded.shape[1])\n",
        "\n",
        "loss_values = [] # a list to keep track of how the loss value changes\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Training loop\n",
        "for _ in trange(1000):\n",
        "  # Your turn: calculate the gradient of MSE with respect to w:\n",
        "  gradient = <YOUR CODE>\n",
        "\n",
        "  # Automatic checks\n",
        "  assert gradient.shape == w.shape\n",
        "  assert (\n",
        "      ((X_expanded @ w - y)**2).mean() > \n",
        "      ((X_expanded @ (w - 1.e-6 * gradient) - y)**2).mean()\n",
        "  )\n",
        "\n",
        "  # Gradient descent step\n",
        "  w -= learning_rate * gradient\n",
        "\n",
        "  # Calculate and record the new loss value\n",
        "  loss_values.append(\n",
        "      ((X_expanded @ w - y)**2).mean()\n",
        "  )\n",
        "\n",
        "# Plotting the evolution of loss values\n",
        "plt.plot(loss_values);\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Plotting the solution\n",
        "x = np.linspace(-1, 1, 101)\n",
        "x_expanded = np.concatenate([\n",
        "    poly_expand.transform(x[:,None]),\n",
        "    np.ones(shape=(len(x), 1))\n",
        "], axis=1)\n",
        "plt.plot(x, true_function(x), label='true function')\n",
        "plt.scatter(X, y);\n",
        "plt.plot(x,\n",
        "         x_expanded @ w, label='prediction')\n",
        "plt.ylim(y.min() - 0.5, y.max() + 0.5)\n",
        "plt.legend();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBenTP_0oDB-",
        "colab_type": "text"
      },
      "source": [
        "Did you notice that numeric solution is less prone to overfitting? Some intuition for that can be found in this post: https://distill.pub/2017/momentum/ (though not explicitly)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbB8SIPiKLp1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}