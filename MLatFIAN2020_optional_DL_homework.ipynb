{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLatFIAN2020-optional-DL-homework.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNNFZjJPn9fxo8/CcmVvzRX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HSE-LAMBDA/MLatFIAN2020/blob/master/MLatFIAN2020_optional_DL_homework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8xA06uIdAPf"
      },
      "source": [
        "# Optional homework: from Logistic Regression to MLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEqbgxlUc6DN"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDjmk66BdPA0"
      },
      "source": [
        "We'll consider the XOR problem - that is a notable example of a simple problem that cannot be solved with a linear classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuXT7goedHSn"
      },
      "source": [
        "X = np.random.uniform(-1, 1, size=(5000, 2))\n",
        "y = ((X.T[0] >= 0.) ^ (X.T[1] >= 0.)).astype('int32')\n",
        "\n",
        "plt.scatter(*X.T, c=y, s=0.4, alpha=0.8, cmap='bwr');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMrg7nvVitJH"
      },
      "source": [
        "Helper function to iterate through batches of data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tniBg_1EeIR2"
      },
      "source": [
        "def sample_batch(X, y, batch_size):\n",
        "  assert len(X) == len(y)\n",
        "  idx = np.random.choice(len(X), len(X), replace=False)\n",
        "  shuffled_X = X[idx]\n",
        "  shuffled_y = y[idx]\n",
        "\n",
        "  for i in range(0, len(X), batch_size):\n",
        "    yield (shuffled_X[i : i + batch_size], shuffled_y[i : i + batch_size])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_A-Gd84iw6G"
      },
      "source": [
        "**Step 1:** Define the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2sSe56ii4VV"
      },
      "source": [
        "# At first, try just a single Dense layer without activation\n",
        "# (that will result in a simple linear regression model)\n",
        "# \n",
        "# After having trained it, come back and make your model more complicated\n",
        "# (add more layers and activations)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    # <YOUR CODE>\n",
        "])\n",
        "\n",
        "\n",
        "# Automatic checks:\n",
        "assert isinstance(model, tf.keras.Sequential), 'Your model should be an instance of tf.keras.Sequential'\n",
        "dummy_pred = model.predict([[-7.04634833, -2.39160895],\n",
        "                            [ 0.09212291,  7.40897428],\n",
        "                            [ 2.53335199, -2.70660191],\n",
        "                            [ 3.62886011, -4.02756296],\n",
        "                            [ 2.7433485 , -1.10504784],\n",
        "                            [ 3.99561633, -8.68322612],\n",
        "                            [ 8.13889866, -8.92227882],\n",
        "                            [-0.10157622,  4.26008939],\n",
        "                            [ 6.10780474,  7.75495299],\n",
        "                            [-4.96919624,  3.83381552]])\n",
        "assert dummy_pred.shape == (10, 1), 'The last layer needs only a single output (since we are doing binary classification)'\n",
        "assert isinstance(model.layers[-1], tf.keras.layers.Dense), \"Why isn't your last layer Dense? o0\"\n",
        "assert model.layers[-1].activation is tf.keras.activations.linear, \"No activation needed in the last layer. We'll combine CrossEntropy and Sigmoid activation in the loss function\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtdeQ8FplKxm"
      },
      "source": [
        "**Step 2:** Define the loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGfXOQ07lNZo"
      },
      "source": [
        "loss_fn = <YOUR CODE> # Cross-entropy loss function with signature:\n",
        "                      #      (y_true, y_pred) -> loss_value\n",
        "                      # where y_true are labels (0 or 1), y_pred are logit predictions, i.e.\n",
        "                      # the predicted probability is `sigmoid(y_pred)`.\n",
        "                      # Make sure to return a scalar (average the loss over predictions).\n",
        "                      #\n",
        "                      # Hint: check out the losses available in `tf.losses`\n",
        "                      # Alternatively, you can define it explicitly as:\n",
        "                      #   loss_fn = lambda y_true, y_pred: ...\n",
        "\n",
        "# Automatic checks:\n",
        "dummy_y_true = tf.convert_to_tensor([0, 0, 1, 1, 0, 1])\n",
        "dummy_y_pred = tf.convert_to_tensor([-7.04634833, -2.39160895, 0.09212291, 7.40897428, 2.53335199, -2.70660191])\n",
        "with tf.GradientTape() as t:\n",
        "  t.watch(dummy_y_pred)\n",
        "  dummy_loss_value = loss_fn(dummy_y_true, dummy_y_pred)\n",
        "dummy_grads = t.gradient(dummy_loss_value, dummy_y_pred)\n",
        "\n",
        "assert isinstance(dummy_loss_value, tf.Tensor)\n",
        "assert dummy_loss_value.shape == []\n",
        "assert np.isclose(dummy_loss_value.numpy(), 1.01969)\n",
        "assert np.allclose(dummy_grads.numpy(), [1.4497087e-04, 1.3969133e-02, -7.9497591e-02, -1.0090417e-04, 1.5440786e-01, -1.5623584e-01])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXfMhUgnpBhn"
      },
      "source": [
        "**Step 3:** Run the training loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SR4GlxiupQoB"
      },
      "source": [
        "I'm providing a ready to use training loop here, but feel free to ignore my example and write it from scratch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEvhefY0dkpE"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "opt = tf.optimizers.Adam()\n",
        "\n",
        "num_epochs = 200\n",
        "batch_size = 512\n",
        "\n",
        "losses = []\n",
        "for i_epoch in range(num_epochs):\n",
        "  if (i_epoch + 1) % 10 == 0:\n",
        "    print(\"Epoch:\", i_epoch + 1)\n",
        "\n",
        "  epoch_loss = 0\n",
        "  for X_batch, y_batch in sample_batch(X, y, batch_size=batch_size):\n",
        "    with tf.GradientTape() as t:\n",
        "      loss_batch = loss_fn(y_batch[:,None], model(X_batch, training=True))\n",
        "    grads = t.gradient(loss_batch, model.trainable_variables)\n",
        "    opt.apply_gradients(zip(grads, model.trainable_variables))\n",
        "    epoch_loss += loss_batch * len(X_batch)\n",
        "\n",
        "  losses.append(epoch_loss.numpy() / len(X))\n",
        "\n",
        "  if (i_epoch + 1) % 100 == 0:\n",
        "    clear_output()\n",
        "    plt.figure()\n",
        "    plt.plot(losses)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OHMZ901pcsx"
      },
      "source": [
        "Check out the prediction:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VXtJEVgeE-B"
      },
      "source": [
        "xx0, xx1 = np.meshgrid(\n",
        "    np.linspace(-1, 1, 100),\n",
        "    np.linspace(-1, 1, 100)\n",
        ")\n",
        "\n",
        "yy = model.predict(np.c_[xx0.ravel(), xx1.ravel()]).reshape(xx0.shape)\n",
        "\n",
        "plt.contourf(xx0, xx1, yy, cmap='bwr', alpha=0.5, levels=30)\n",
        "plt.scatter(*X.T, c=y, s=0.4, alpha=0.8, cmap='bwr');\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yx2ggPLPqwMo"
      },
      "source": [
        "What predictions do you get with no hidden layers (simple logistic regression model)?\n",
        "\n",
        "Try other things:\n",
        "\n",
        "- Single hidden layer:\n",
        "\n",
        "  - Try adding a single hidden layer with 100 units and an activation function (e.g. ELU) and run the training again. Does it get better?\n",
        "\n",
        "  - Play around with the number of neurons in the hidden layer.\n",
        "    - Try very small numbers (e.g. 1-2 neurons in the hidden layer). Does it work?\n",
        "    - Is it possible to solve this problem with just 1 neuron in the hidden layer?\n",
        "    - Can you think of a theoretical solution with 2 neurons in the hidden layer? (Hint: it exists.) Were you able to find a solution by training your model with a gradient based method?\n",
        "\n",
        "- MLP:\n",
        "  - Play around with the number of hidden layers\n",
        "  - What is better (from the training perspective) - to have more layers (deeper network) or to have more neurons per layer (wider network)?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1rpQk3wpenE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}