{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLatFIAN2020-seminar11-AutoencodersAndSemiSupervised.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HSE-LAMBDA/MLatFIAN2020/blob/master/seminar11/MLatFIAN2020_seminar11_AutoencodersAndSemiSupervised.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLFslUEjAQV-"
      },
      "source": [
        "# Autoencoder example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqjmKz_qATNL"
      },
      "source": [
        "In this example we'll show how an auto-encoder can help when labeled data is limited. Will take the MNIST data keeping only the labels of first 300 examples (out of 60000)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVHvLYbQyEQs"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORQkf4J0Ablq"
      },
      "source": [
        "Load and preprocess the data (as numpy arrays):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91UFnB0FyaUA"
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "X_train = (X_train / 255).astype('float32')\n",
        "X_test  = (X_test  / 255).astype('float32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vU8aXx0iAfy1"
      },
      "source": [
        "Basic autoencoder architecture:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZLrAP33y0Sx"
      },
      "source": [
        "ll = tf.keras.layers\n",
        "activation = tf.nn.relu\n",
        "\n",
        "encoder = tf.keras.Sequential([\n",
        "    ll.Reshape((1, 28, 28), input_shape=(28, 28)),\n",
        "\n",
        "    ll.Conv2D(16, 3, padding='same', data_format='channels_first', activation=activation),\n",
        "    ll.MaxPool2D(data_format='channels_first'), # 14x14\n",
        "\n",
        "    ll.Conv2D(32, 3, padding='same', data_format='channels_first', activation=activation),\n",
        "    ll.MaxPool2D(data_format='channels_first'), # 7x7\n",
        "\n",
        "    ll.Conv2D(64, 3, padding='valid', data_format='channels_first', activation=activation), # 5x5\n",
        "    ll.Conv2D(128, 3, padding='valid', data_format='channels_first', activation=activation), # 3x3\n",
        "    ll.Conv2D(256, 3, padding='valid', data_format='channels_first', activation=activation), # 1x1\n",
        "    ll.Conv2D(32, 1, padding='same', data_format='channels_first', activation=activation),\n",
        "\n",
        "    ll.Reshape((32,))\n",
        "  ],\n",
        "  name='encoder')\n",
        "\n",
        "decoder = tf.keras.Sequential([\n",
        "    ll.Reshape((32, 1, 1), input_shape=(32,)),\n",
        "\n",
        "    ll.UpSampling2D(data_format='channels_first'), # 2x2\n",
        "    ll.Conv2D(256, 3, padding='same', data_format='channels_first', activation=activation),\n",
        "\n",
        "    ll.UpSampling2D(data_format='channels_first'), # 4x4\n",
        "    ll.Conv2D(128, 3, padding='same', data_format='channels_first', activation=activation),\n",
        "\n",
        "    ll.UpSampling2D(data_format='channels_first'), # 8x8\n",
        "    ll.Conv2D(64, 3, padding='same', data_format='channels_first', activation=activation),\n",
        "\n",
        "    ll.UpSampling2D(data_format='channels_first'), # 16x16\n",
        "    ll.Conv2D(32, 3, padding='valid', data_format='channels_first', activation=activation), # 14x14\n",
        "\n",
        "    ll.UpSampling2D(data_format='channels_first'), # 28x28\n",
        "    ll.Conv2D(16, 3, padding='same', data_format='channels_first', activation=activation),\n",
        "    ll.Conv2D(1, 1, padding='valid', data_format='channels_first', activation=tf.nn.relu),\n",
        "\n",
        "    ll.Reshape((28, 28))\n",
        "  ],\n",
        "  name='decoder')\n",
        "\n",
        "encoder.save('encoder_untrained.h5')\n",
        "\n",
        "autoencoder = tf.keras.Sequential([\n",
        "  encoder,\n",
        "  decoder\n",
        "])\n",
        "\n",
        "autoencoder.summary()\n",
        "autoencoder.compile(optimizer=tf.optimizers.Adam(learning_rate=0.003), loss=tf.keras.losses.MSE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmpG6LSzA-H5"
      },
      "source": [
        "Trainign it on the whole train dataset (note that `X_train` is both inputs and targets for the autoencoder, i.e. we are not using the labels `y_train`):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2w81dMJ5f04"
      },
      "source": [
        "autoencoder.fit(x=X_train, y=X_train, batch_size=256, epochs=16, validation_data=(X_test, X_test));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYpVWaaCBLLC"
      },
      "source": [
        "Some plotting code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cA72rNfw5yBw"
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwNCF8Mv6bmU"
      },
      "source": [
        "plt.plot(autoencoder.history.history['loss'], label='train loss')\n",
        "plt.plot(autoencoder.history.history['val_loss'], label='test loss')\n",
        "plt.legend()\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vSAskFR2-Sh"
      },
      "source": [
        "np.random.seed(23864)\n",
        "idx = np.random.randint(len(X_test), size=100)\n",
        "rec_X_test = autoencoder(X_test, training=False).numpy()\n",
        "\n",
        "def plot100(imgs):\n",
        "  plt.imshow(\n",
        "      np.array(imgs).reshape(\n",
        "          (10, 10) + imgs.shape[1:3]\n",
        "      ).transpose(0, 2, 1, 3).reshape(np.array(imgs.shape[1:]) * 10)\n",
        "  )\n",
        "  plt.axis('off')\n",
        "\n",
        "plt.figure(figsize=(10, 5), dpi=100)\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"Original\")\n",
        "plot100(X_test[idx])\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"Reconstructed\")\n",
        "plot100(rec_X_test[idx])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7w_BV5sS4Lq"
      },
      "source": [
        "### Exercise: interpolating between bottleneck representations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GTAj_i-4ap7"
      },
      "source": [
        "digit_1 = 5\n",
        "digit_2 = 1\n",
        "\n",
        "# Let's take 10 images of digit_1 and digit_2:\n",
        "digits_1 = X_test[y_test == digit_1][:10]\n",
        "digits_2 = X_test[y_test == digit_2][:10]\n",
        "\n",
        "# Calculate the encoded representations of these digits:\n",
        "representation_1 = <YOUR CODE>\n",
        "representation_2 = <YOUR CODE>\n",
        "\n",
        "# Now create a 10x10x<BOTTLENECK_SIZE> matrix of linear interpolations between\n",
        "# the two representations:\n",
        "representation_mixed = <YOUR CODE>\n",
        "\n",
        "# Then decode the images from the mixed representations:\n",
        "mixed_imgs = <YOUR CODE>\n",
        "\n",
        "plt.figure(figsize=(6, 6), dpi=100)\n",
        "plot100(mixed_imgs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcHTdJoNUKj_"
      },
      "source": [
        "# Training a classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75c3YT2aBN3p"
      },
      "source": [
        "Now we'll use the encoder to train a classifier on its outputs. We don't want to update the encoder's weights anymore:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ah9prSd77tXP"
      },
      "source": [
        "encoder.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NZu6dTI72Ye"
      },
      "source": [
        "classifier = tf.keras.Sequential([\n",
        "    encoder,\n",
        "    ll.Dense(10)\n",
        "  ],\n",
        "  name='classifier')\n",
        "\n",
        "classifier.summary()\n",
        "classifier.compile(optimizer=tf.optimizers.RMSprop(learning_rate=0.05), loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                   metrics=[tf.metrics.SparseCategoricalAccuracy()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YU0q6A-dBawA"
      },
      "source": [
        "Now let's fit the classifier on first 300 labeled objects:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzJe7qVq9UEN"
      },
      "source": [
        "classifier.fit(x=X_train[:300], y=y_train[:300], batch_size=128, epochs=50, validation_data=(X_test, y_test));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LA5OCr-KButF"
      },
      "source": [
        "We got test accuracy up to 90% by training on just 300 objects! Note that there is 10000 objects in the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uBxiqLS9lba"
      },
      "source": [
        "plt.figure(figsize=(16, 7))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(classifier.history.history['loss'], label='train')\n",
        "plt.plot(classifier.history.history['val_loss'], label='test')\n",
        "plt.legend()\n",
        "plt.title('Cross-entropy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(classifier.history.history['sparse_categorical_accuracy'], label='train')\n",
        "plt.plot(classifier.history.history['val_sparse_categorical_accuracy'], label='test')\n",
        "plt.legend()\n",
        "plt.title('Accuracy')\n",
        "plt.show();\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CW-XJk21B-2O"
      },
      "source": [
        "Now let's imagine we have similar architecture to train from scratch on just 300 objects:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JEs5ce0_mii"
      },
      "source": [
        "encoder.trainable = True\n",
        "encoder.load_weights('encoder_untrained.h5')\n",
        "\n",
        "classifier = tf.keras.Sequential([\n",
        "  encoder,\n",
        "  ll.Dense(10),\n",
        "  ],\n",
        "  name='classifier')\n",
        "\n",
        "classifier.summary()\n",
        "classifier.compile(optimizer=tf.optimizers.Adam(learning_rate=0.01), loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                   metrics=[tf.metrics.SparseCategoricalAccuracy()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnKHwckQAUR_"
      },
      "source": [
        "classifier.fit(x=X_train[:300], y=y_train[:300], batch_size=64, epochs=50, validation_data=(X_test, y_test));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ERclRvvCWwi"
      },
      "source": [
        "This results in a heavy overfit:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6k8B15e9Ajyv"
      },
      "source": [
        "plt.figure(figsize=(16, 7))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(classifier.history.history['loss'], label='train')\n",
        "plt.plot(classifier.history.history['val_loss'], label='test')\n",
        "plt.legend()\n",
        "plt.title('Cross-entropy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(classifier.history.history['sparse_categorical_accuracy'], label='train')\n",
        "plt.plot(classifier.history.history['val_sparse_categorical_accuracy'], label='test')\n",
        "plt.legend()\n",
        "plt.title('Accuracy')\n",
        "plt.show();\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5-oJn2BCh0H"
      },
      "source": [
        "# Simultaneously optimizing the AE and classifier\n",
        "\n",
        "Before running this code, make sure to re-initialize the autoencoder and the classifier network (run corresponding cells above)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDk0sg5RFpNy"
      },
      "source": [
        "X_train_labeled, X_train_unlabeled = X_train[:300], X_train[300:]\n",
        "y_train_labeled = y_train[:300]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQTduJu9GJyC"
      },
      "source": [
        "from itertools import cycle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSObimjOF75w"
      },
      "source": [
        "def gen_unlabeled(batch_size):\n",
        "  ids = np.arange(len(X_train_unlabeled))\n",
        "  np.random.shuffle(ids)\n",
        "  shuffled_X_train_unlabeled = X_train_unlabeled[ids]\n",
        "  for i in range(0, len(X_train_unlabeled), batch_size):\n",
        "    yield shuffled_X_train_unlabeled[i:i+batch_size]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rlo13l-IG7-X"
      },
      "source": [
        "len(X_train_unlabeled)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTxnh5jYGo09"
      },
      "source": [
        "unlabeled_generator = iter(cycle(gen_unlabeled(256)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wr-MqV3jIcOk"
      },
      "source": [
        "classifier.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKIGXJGyKhqa"
      },
      "source": [
        "variables = classifier.trainable_variables + decoder.trainable_variables"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seUHD4IFLF8x"
      },
      "source": [
        "opt = tf.optimizers.Adam()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pixdTHxVLqYO"
      },
      "source": [
        "from IPython.display import clear_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4q_rrngHKkA"
      },
      "source": [
        "N_EPOCHS = 100\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "LAMBDA = 0.1\n",
        "history_ae = []\n",
        "history_cl = []\n",
        "history_tot = []\n",
        "for i_epoch in range(N_EPOCHS):\n",
        "  print(\"Working on ep #\", i_epoch)\n",
        "  ids = np.arange(len(X_train_labeled))\n",
        "  np.random.shuffle(ids)\n",
        "\n",
        "  epoch_ae_loss = 0\n",
        "  epoch_cl_loss = 0\n",
        "  epoch_total_loss = 0\n",
        "\n",
        "  for i_image in range(0, len(X_train_labeled), BATCH_SIZE):\n",
        "    X_batch = X_train_labeled[ids][i_image:i_image + BATCH_SIZE]\n",
        "    y_batch = y_train_labeled[ids][i_image:i_image + BATCH_SIZE]\n",
        "    X_batch_unlabeled = next(unlabeled_generator)\n",
        "\n",
        "    with tf.GradientTape() as t:\n",
        "      reco = decoder(encoder(X_batch_unlabeled))\n",
        "      preds = classifier(X_batch)\n",
        "\n",
        "      ae_loss = tf.reduce_mean(tf.reduce_sum((X_batch_unlabeled - reco)**2, axis=(1, 2)))\n",
        "      cl_loss = tf.reduce_mean(tf.losses.sparse_categorical_crossentropy(y_batch, preds, from_logits=True))\n",
        "\n",
        "      loss = cl_loss + LAMBDA * ae_loss\n",
        "    epoch_ae_loss += ae_loss.numpy()\n",
        "    epoch_cl_loss += cl_loss.numpy()\n",
        "    epoch_total_loss += loss.numpy()\n",
        "    grads = t.gradient(loss, variables)\n",
        "    opt.apply_gradients(zip(grads, variables))\n",
        "  history_ae.append(epoch_ae_loss / len(X_train_labeled))\n",
        "  history_cl.append(epoch_cl_loss / len(X_train_labeled))\n",
        "  history_tot.append(epoch_total_loss / len(X_train_labeled))\n",
        "\n",
        "  if i_epoch % 10 == 0:\n",
        "    clear_output(wait=True)\n",
        "    plt.figure(figsize=(9, 6))\n",
        "    plt.plot(history_ae, label='ae loss')\n",
        "    plt.plot(history_cl, label='cl loss')\n",
        "    plt.plot(history_tot, label='total')\n",
        "    plt.legend()\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('loss')\n",
        "    plt.show();\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaTmdc71NXuS"
      },
      "source": [
        "plt.figure(figsize=(9, 6))\n",
        "plt.plot(history_ae, label='ae loss')\n",
        "plt.plot(history_cl, label='cl loss')\n",
        "plt.plot(history_tot, label='total')\n",
        "plt.legend()\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRt_V_LPJoUf"
      },
      "source": [
        "preds_train = classifier(X_train).numpy().argmax(axis=1)\n",
        "preds_test  = classifier(X_test ).numpy().argmax(axis=1)\n",
        "\n",
        "print((preds_train == y_train).mean())\n",
        "print((preds_test == y_test).mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INpJR-_BSUQp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}